{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1a19de",
   "metadata": {},
   "source": [
    "# Applying the Hidden Markov Model on Budget allocation\n",
    "\n",
    "The hidden markov model would recommend a budget using conversion state transitions, budget emissions, and return on investment indicators \n",
    "\n",
    "### Objective\n",
    "Minimize the budget _B<sub>t</sub>_ to transition from state _S<sub>t</sub>_ to _S<sub>t+1</sub>_\n",
    "\n",
    "### Subject to\n",
    "* Sum of _B<sub>t</sub>_ <= _B_;  for all _t_\n",
    "* RMS of _R_ <= $\\epsilon$; where _R_ is the Residual of the expected and actual return on investmenet _B_\n",
    "* Sum of _S<sub>t+1</sub> - S<sub>t</sub>_ <= _T_; where _T_ is the total allowable time period to complete all transitions from S<sub>start</sub> to S<sub>final</sub>\n",
    "\n",
    "#### Reference\n",
    "[Analyzing Time Series Data with Markov Transition Matrices](https://medium.com/towards-data-science/time-series-data-markov-transition-matrices-7060771e362b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2801e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bda64",
   "metadata": {},
   "source": [
    "## Instantiate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e9a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 13:09:13.302701: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-13 13:09:15.249253: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-13 13:09:16.390989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739423357.407988  318893 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739423357.614501  318893 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 13:09:19.496428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional SPARKRDBM-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKFILE-libraries in LOADER-package of ETL-module imported successfully!\n",
      "__propAttr__ Class initialization complete\n",
      "__propAttr__ Class initialization complete\n",
      "sparkFile Class initialization complete\n",
      "\n",
      "read and write BigQuery dataset for hypothese testing class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "\n",
    "proj_dir = os.path.abspath(os.pardir)\n",
    "sys.path.insert(1,proj_dir.split('mining/')[0])\n",
    "from rezaware.modules.etl.loader import sparkRDBM as db\n",
    "from rezaware.modules.etl.loader import sparkFile as file\n",
    "# from rezaware.modules.etl.loader import __propAttr__ as attr\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    db = importlib.reload(db)\n",
    "    file=importlib.reload(file)\n",
    "    # attr=importlib.reload(attr)\n",
    "\n",
    "__desc__ = \"read and write BigQuery dataset for hypothese testing\"\n",
    "clsSDB = db.dataWorkLoads(\n",
    "    desc=__desc__,\n",
    "    db_type = 'bigquery',\n",
    "    db_driver=None,\n",
    "    db_hostIP=None,\n",
    "    db_port = None,\n",
    "    db_name = None,\n",
    "    db_schema='combined_data_facebook_ads',\n",
    "    spark_partitions=None,\n",
    "    spark_format = 'bigquery',\n",
    "    spark_save_mode=None,\n",
    "    # spark_jar_dir = _jar,\n",
    ")\n",
    "clsFile = file.dataWorkLoads(\n",
    "    desc = \"optimizing action_type budgets for an ad\",\n",
    "    store_mode='local-fs',\n",
    "    store_root=proj_dir.split('mining/')[0],\n",
    "    jar_dir=None,\n",
    ")\n",
    "# if clsSDB.session:\n",
    "#     clsSDB._session.stop\n",
    "print(\"\\n%s class initialization and load complete!\" % __desc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8066d7-a018-4f32-aceb-b0f35fb1d3d2",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958f7a62-0f2c-4908-a0a1-a2c0a42f9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 10:36:40 WARN Utils: Your hostname, Waidy-Think-Three resolves to a loopback address: 127.0.1.1; using 192.168.2.82 instead (on interface enp0s25)\n",
      "25/02/13 10:36:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/02/13 10:36:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61513 rows\n",
      "root\n",
      " |-- account_id: long (nullable = true)\n",
      " |-- ad_id: long (nullable = true)\n",
      " |-- adset_id: long (nullable = true)\n",
      " |-- campaign_id: long (nullable = true)\n",
      " |-- updated_time: date (nullable = true)\n",
      " |-- impressions: integer (nullable = true)\n",
      " |-- frequency: double (nullable = true)\n",
      " |-- reach: integer (nullable = true)\n",
      " |-- CTR: double (nullable = true)\n",
      " |-- CPC: double (nullable = true)\n",
      " |-- CPM: double (nullable = true)\n",
      " |-- ROAS: double (nullable = true)\n",
      " |-- social_spend: double (nullable = true)\n",
      " |-- spend: double (nullable = true)\n",
      " |-- click: integer (nullable = true)\n",
      " |-- auction_bid: integer (nullable = true)\n",
      " |-- goal: string (nullable = true)\n",
      " |-- purchase_value: double (nullable = true)\n",
      " |-- account_currency: string (nullable = true)\n",
      " |-- purchase: double (nullable = true)\n",
      " |-- fb_pixel_view_value: double (nullable = true)\n",
      " |-- fb_pixel_purchase_value: double (nullable = true)\n",
      " |-- fb_pixel_add_to_cart_value: double (nullable = true)\n",
      " |-- mobile_app_purchase_value: string (nullable = true)\n",
      " |-- link_click_value: string (nullable = true)\n",
      " |-- landing_page_view_value: string (nullable = true)\n",
      " |-- like_value: string (nullable = true)\n",
      " |-- comment_value: string (nullable = true)\n",
      " |-- post_reaction_value: string (nullable = true)\n",
      " |-- video_view_value: string (nullable = true)\n",
      " |-- app_install_value: string (nullable = true)\n",
      " |-- outbound_click_value: string (nullable = true)\n",
      " |-- engagement_value: string (nullable = true)\n",
      " |-- omni_purchase_value: double (nullable = true)\n",
      " |-- omni_view_content_value: double (nullable = true)\n",
      " |-- omni_add_to_cart_value: double (nullable = true)\n",
      " |-- omni_initiated_checkout_value: double (nullable = true)\n",
      " |-- omni_search_value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    \"inferSchema\":True,\n",
    "    \"header\":True,\n",
    "    \"delimiter\":\",\",\n",
    "    \"pathGlobFilter\":'*.csv',\n",
    "    \"recursiveFileLookup\":True,\n",
    "}\n",
    "\n",
    "sdf=clsFile.read_files_to_dtype(\n",
    "    as_type = \"SPARK\",      # optional - define the data type to return\n",
    "    folder_path=\"mining/data/budget/\",  # optional - relative path, w.r.t. self.storeRoot\n",
    "        file_name=\"complete-60-accounts.csv\",  # optional - name of the file to read\n",
    "        file_type=None,  # optional - read all the files of same type\n",
    "        **options,\n",
    ")\n",
    "print(\"Loaded %d rows\" % sdf.count())\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87c6b548-c585-4847-812b-a1807f0fb2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, ('omni_view_content_value', 'omni_add_to_cart_value')), (1, ('omni_view_content_value', 'omni_initiated_checkout_value')), (2, ('omni_add_to_cart_value', 'omni_purchase_value')), (3, ('omni_initiated_checkout_value', 'omni_purchase_value'))])\n",
      "['omni_initiated_checkout_value', 'omni_purchase_value', 'omni_view_content_value', 'omni_add_to_cart_value']\n",
      "omni_initiated_checkout_value omni_initiated_checkout_value\n",
      "omni_initiated_checkout_value omni_purchase_value\n",
      "omni_initiated_checkout_value omni_view_content_value\n",
      "omni_initiated_checkout_value omni_add_to_cart_value\n",
      "omni_purchase_value omni_initiated_checkout_value\n",
      "omni_purchase_value omni_purchase_value\n",
      "omni_purchase_value omni_view_content_value\n",
      "omni_purchase_value omni_add_to_cart_value\n",
      "omni_view_content_value omni_initiated_checkout_value\n",
      "omni_view_content_value omni_purchase_value\n",
      "omni_view_content_value omni_view_content_value\n",
      "omni_view_content_value omni_add_to_cart_value\n",
      "omni_add_to_cart_value omni_initiated_checkout_value\n",
      "omni_add_to_cart_value omni_purchase_value\n",
      "omni_add_to_cart_value omni_view_content_value\n",
      "omni_add_to_cart_value omni_add_to_cart_value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "# _state_trans = [\n",
    "#     ('omni_view_content_value','omni_add_to_cart_value'),\n",
    "#     ('omni_view_content_value', 'omni_initiated_checkout_value'),\n",
    "#     ('omni_add_to_cart_value', 'omni_purchase_value'),\n",
    "#     ('omni_initiated_checkout_value', 'omni_purchase_value'),\n",
    "# ]\n",
    "_state_trans = {\n",
    "    0:('omni_view_content_value','omni_add_to_cart_value'),\n",
    "    1:('omni_view_content_value', 'omni_initiated_checkout_value'),\n",
    "    2:('omni_add_to_cart_value', 'omni_purchase_value'),\n",
    "    3:('omni_initiated_checkout_value', 'omni_purchase_value'),\n",
    "}\n",
    "_state_trans=collections.OrderedDict(sorted(_state_trans.items()))\n",
    "print(_state_trans)\n",
    "''' discover the states from the transition meta data '''\n",
    "_states = list(set(list(chain(*[_state_trans[k] for k in _state_trans.keys()]))))\n",
    "print(_states)\n",
    "_trans_matrix = np.zeros((len(_states),len(_states)))\n",
    "\n",
    "for i in range(0,len(_states),1):\n",
    "    for j in range(0,len(_states),1):\n",
    "        print(_states[i],_states[j])\n",
    "        if (_states[i],_states[j]) in _state_trans:\n",
    "            # print(_states[i],_states[j])\n",
    "            _trans_matrix[i,j]=10\n",
    "_trans_matrix\n",
    "\n",
    "# _max_states = len(_state_trans)\n",
    "\n",
    "# for _trans in _state_trans:\n",
    "#     _from_state, _to_state = _state_trans[_trans][0], _state_trans[_trans][1]\n",
    "#     print(_from_state, _to_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8dc0283-0c8b-44fa-9ec4-71da2983aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+-----------------------------+-------------------+\n",
      "|omni_view_content_value|omni_add_to_cart_value|omni_initiated_checkout_value|omni_purchase_value|\n",
      "+-----------------------+----------------------+-----------------------------+-------------------+\n",
      "|661.0                  |61.0                  |61.0                         |155.13             |\n",
      "|150.02                 |null                  |null                         |null               |\n",
      "|289.0                  |44.0                  |null                         |null               |\n",
      "+-----------------------+----------------------+-----------------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "_actions=sdf.select('omni_view_content_value', 'omni_add_to_cart_value',\n",
    "                    'omni_initiated_checkout_value', 'omni_purchase_value')\\\n",
    "            .orderBy(F.col('updated_time'))\n",
    "_actions.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502f5eb-959c-461a-85b0-fadcd9caf292",
   "metadata": {},
   "source": [
    "## Implement the Belman equation\n",
    "The [Belman equation](https://www.datacamp.com/tutorial/bellman-equation-reinforcement-learning) has the following componensts:\n",
    "1. R(s,a): The immediate reward received for taking action aaa in state sss.\n",
    "2. γ: The discount factor (between 0 and 1) that determines the importance of future rewards compared to immediate rewards.\n",
    "3. P(s′∣s,a) The probability of transitioning to state s′ from state sss by taking action a.\n",
    "4. max⁡(a): The optimal action that maximizes the expected value of future rewards.\n",
    "\n",
    "### Filter and Separate dataset\n",
    "1. Transition matrix features and values\n",
    "2. Emission matrix features and values\n",
    "3. Observed outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86cd5e3b-0379-4119-936b-973eae1cd8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_emissions\n",
      " [[18.63664596  0.92        1.238938   11.39823   ]\n",
      " [18.19        1.          1.322751   13.227513  ]\n",
      " [ 9.85487616  1.845714    1.592719   29.397042  ]\n",
      " ...\n",
      " [ 0.35527015  0.836296    3.602882   30.130771  ]\n",
      " [ 2.24693786  0.972609    3.141218   30.551762  ]\n",
      " [ 5.1682328   0.785476    3.050109   23.957879  ]] (26193, 4)\n",
      "\n",
      "_observes\n",
      " [[3.7302e+02 2.2400e+02 1.2002e+02]\n",
      " [2.1804e+02 8.4000e+01 9.0950e+01]\n",
      " [5.4508e+02 2.4200e+02 2.5465e+02]\n",
      " ...\n",
      " [9.2950e+01 5.7970e+01 4.0980e+01]\n",
      " [2.0000e-02 8.6000e+01 1.2835e+02]\n",
      " [9.0400e+02 1.6000e+02 1.0625e+02]] 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_emissions=np.array(sdf.select('ROAS','CPC','CTR','CPM').dropna().collect())\n",
    "print(\"_emissions\\n\",_emissions,_emissions.shape)\n",
    "\n",
    "_observs=np.array(sdf.select('fb_pixel_view_value','fb_pixel_add_to_cart_value',\n",
    "                             'fb_pixel_purchase_value').dropna().collect())\n",
    "print('\\n_observes\\n',_observs,_observs.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19631939-4685-4cdd-91e4-d055fa3d98c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([373.02, 224.  , 120.02])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_observs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c4e7200-e148-4612-b362-bb3112cc1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Symbol counts should be nonnegative integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhmmlearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhmm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultinomialHMM\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m MultinomialHMM(\n\u001b[1;32m      4\u001b[0m     n_components\u001b[38;5;241m=\u001b[39m_emissions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      5\u001b[0m     startprob_prior\u001b[38;5;241m=\u001b[39m_observs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_observs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/madgicx-63NknB0J-py3.10/lib/python3.10/site-packages/hmmlearn/base.py:480\u001b[0m, in \u001b[0;36m_AbstractHMM.fit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lengths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor_\u001b[38;5;241m.\u001b[39m_reset()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/madgicx-63NknB0J-py3.10/lib/python3.10/site-packages/hmmlearn/hmm.py:919\u001b[0m, in \u001b[0;36mMultinomialHMM._init\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_params:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/madgicx-63NknB0J-py3.10/lib/python3.10/site-packages/hmmlearn/base.py:928\u001b[0m, in \u001b[0;36mBaseHMM._init\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    920\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m    Initialize model parameters prior to fitting.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03m        Feature matrix of individual samples.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 928\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_set_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     init \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\n\u001b[1;32m    930\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/madgicx-63NknB0J-py3.10/lib/python3.10/site-packages/hmmlearn/_emissions.py:319\u001b[0m, in \u001b[0;36mBaseMultinomialHMM._check_and_set_n_features\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_check_and_set_n_features(X)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(X\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbol counts should be nonnegative integers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Symbol counts should be nonnegative integers"
     ]
    }
   ],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM\n",
    "\n",
    "model = MultinomialHMM(\n",
    "    n_components=_emissions.shape[1],\n",
    "    startprob_prior=_observs[0],\n",
    "    algorithm='viterbi',\n",
    "    random_state=0,\n",
    "    n_iter=1,\n",
    ")\n",
    "model.fit(_observs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19f7f8-4e4b-4ad1-ba28-e4a7dc4a256f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
