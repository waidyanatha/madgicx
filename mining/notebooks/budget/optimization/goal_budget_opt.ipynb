{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b2ae34-6ea4-44ea-a964-586c2615200f",
   "metadata": {},
   "source": [
    "# Prediction ML for goal and budget optimization\n",
    "\n",
    "Given an _action type_ sequence of counts and transition probabilities, of a goal at time t, what is the probability that the _spend amount_ will result in filling the expected _action type_ counts at time t+1, t+2, ... ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808e3322-ec14-4682-bee1-4b477eaedb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cddcfcce-391c-4a13-b9e6-a4bd163541d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional __PROPATTR__-libraries in OPTIMIZATION-package of BUDGET-module imported successfully!\n",
      "All functional PREDGOALSPEND-libraries in OPTIMIZATION-package of BUDGET-module imported successfully!\n",
      "All functional SPARKFILE-libraries in LOADER-package of ETL-module imported successfully!\n",
      "__propAttr__ Class initialization complete\n",
      "sparkFile Class initialization complete\n",
      "__propAttr__ Class initialization complete\n",
      "predGoalSpend Class initialization complete\n",
      "\n",
      "read and write BigQuery dataset for hypothese testing class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "\n",
    "proj_dir = os.path.abspath(os.pardir)\n",
    "sys.path.insert(1,proj_dir.split('mining/')[0])\n",
    "from mining.modules.budget.optimization import predGoalSpend as gs\n",
    "from mining.modules.budget.optimization import __propAttr__ as pr\n",
    "from rezaware.modules.etl.loader import sparkFile as file\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    pr = importlib.reload(pr)\n",
    "    gs = importlib.reload(gs)\n",
    "    file=importlib.reload(file)\n",
    "    # attr=importlib.reload(attr)\n",
    "\n",
    "__desc__ = \"read and write BigQuery dataset for hypothese testing\"\n",
    "\n",
    "clsFile = file.dataWorkLoads(\n",
    "    desc = __desc__,\n",
    "    store_mode='local-fs',\n",
    "    store_root=proj_dir.split('mining/')[0],\n",
    "    jar_dir=None,\n",
    ")\n",
    "clsGS = gs.adBudgetMLWorkLoads(desc = __desc__)\n",
    "\n",
    "print(\"\\n%s class initialization and load complete!\" % __desc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954dafb9-1633-42d6-9314-776ba1ae3265",
   "metadata": {},
   "source": [
    "## Train and Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a0a86d9-532d-4829-a585-20be199f051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "# __def_spend_col_name__='spend_usd'\n",
    "# __feature_col_lst__ = ['CPC', 'CPM', 'CTR', 'ROAS', \n",
    "#                        'impressions', 'click', 'reach', 'frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58bbfa9e-514b-40c3-9cda-21ea662e6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed model fit Pipeline(steps=[('transformer', StandardScaler()),\n",
      "                ('numerical_imputer', SimpleImputer()),\n",
      "                ('trained_model',\n",
      "                 ExtraTreesRegressor(n_estimators=200, random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('transformer', StandardScaler()),   # best compared to MinMaxScaler or Polynomial Feature\n",
    "    # ('reduction', PCA(n_components=2)),  # PCA reduction doesn't improve scores\n",
    "    ('numerical_imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('trained_model', ExtraTreesRegressor(n_estimators=200, random_state=0)),\n",
    "])\n",
    "pipe.fit(\n",
    "    X_train, #.reshape(-1,len(__feature_col_lst__)),\n",
    "    y_train, #.reshape(-1,1)\n",
    "    )\n",
    "print(\"Completed model fit\", pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa06ca4-3c48-4093-8795-41942b1ff2a2",
   "metadata": {},
   "source": [
    "### Cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac49c34f-ca79-4bb6-9ab8-cbcfad86a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score mean: 0.03 and stdv: 0.34\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipe, X, y, cv=10)\n",
    "print(\"Score mean: %0.2f and stdv: %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca3174-7895-4f99-b7d1-b2c7a2e28432",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe9b72b7-6c12-4a2e-8bf1-5dee431eceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete shape: (11262,)\n",
      "MAE 17.23009104058782\n",
      "RMSE 47.13084457009876\n"
     ]
    }
   ],
   "source": [
    "# y_pred = pipe.predict(X_test)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"Prediction complete shape:\", y_pred.shape)\n",
    "# print scores\n",
    "print('MAE',mean_absolute_error(y_test, y_pred))\n",
    "print('RMSE',root_mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5a2d4-c6c5-4b23-9738-671528c1e25e",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3424814a-b3be-4e9f-8933-7a7b0aa29333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/21 14:14:40 WARN Utils: Your hostname, Waidy-Think-Three resolves to a loopback address: 127.0.1.1; using 192.168.2.82 instead (on interface enp0s25)\n",
      "25/02/21 14:14:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/02/21 14:14:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/21 14:14:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61513 rows\n",
      "Converted spend to usd\n",
      "root\n",
      " |-- account_id: long (nullable = true)\n",
      " |-- ad_id: long (nullable = true)\n",
      " |-- adset_id: long (nullable = true)\n",
      " |-- campaign_id: long (nullable = true)\n",
      " |-- updated_time: date (nullable = true)\n",
      " |-- impressions: integer (nullable = true)\n",
      " |-- frequency: double (nullable = true)\n",
      " |-- reach: integer (nullable = true)\n",
      " |-- CTR: double (nullable = true)\n",
      " |-- CPC: double (nullable = true)\n",
      " |-- CPM: double (nullable = true)\n",
      " |-- ROAS: double (nullable = true)\n",
      " |-- social_spend: double (nullable = true)\n",
      " |-- spend: double (nullable = true)\n",
      " |-- click: integer (nullable = true)\n",
      " |-- auction_bid: integer (nullable = true)\n",
      " |-- goal: string (nullable = true)\n",
      " |-- purchase_value: double (nullable = true)\n",
      " |-- account_currency: string (nullable = true)\n",
      " |-- purchase: double (nullable = true)\n",
      " |-- fb_pixel_view_value: double (nullable = true)\n",
      " |-- fb_pixel_purchase_value: double (nullable = true)\n",
      " |-- fb_pixel_add_to_cart_value: double (nullable = true)\n",
      " |-- mobile_app_purchase_value: string (nullable = true)\n",
      " |-- link_click_value: string (nullable = true)\n",
      " |-- landing_page_view_value: string (nullable = true)\n",
      " |-- like_value: string (nullable = true)\n",
      " |-- comment_value: string (nullable = true)\n",
      " |-- post_reaction_value: string (nullable = true)\n",
      " |-- video_view_value: string (nullable = true)\n",
      " |-- app_install_value: string (nullable = true)\n",
      " |-- outbound_click_value: string (nullable = true)\n",
      " |-- engagement_value: string (nullable = true)\n",
      " |-- omni_purchase_value: double (nullable = true)\n",
      " |-- omni_view_content_value: double (nullable = true)\n",
      " |-- omni_add_to_cart_value: double (nullable = true)\n",
      " |-- omni_initiated_checkout_value: double (nullable = true)\n",
      " |-- omni_search_value: string (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- spend_usd: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "options = {\n",
    "    \"inferSchema\":True,\n",
    "    \"header\":True,\n",
    "    \"delimiter\":\",\",\n",
    "    \"pathGlobFilter\":'*.csv',\n",
    "    \"recursiveFileLookup\":True,\n",
    "}\n",
    "\n",
    "sdf=clsFile.read_files_to_dtype(\n",
    "    as_type = \"SPARK\",      # optional - define the data type to return\n",
    "    folder_path=\"mining/data/budget/\",  # optional - relative path, w.r.t. self.storeRoot\n",
    "    file_name=\"complete-60-accounts.csv\",  # optional - name of the file to read\n",
    "    file_type=None,  # optional - read all the files of same type\n",
    "    **options,\n",
    ")\n",
    "print(\"Loaded %d rows\" % sdf.count())\n",
    "\n",
    "sdf = sdf.withColumn('date', unix_timestamp('updated_time'))\n",
    "\n",
    "### USE python currency-converter librabry with date\n",
    "sdf = sdf.withColumn(\n",
    "    \"spend_usd\",\n",
    "    F.when(F.col('account_currency').isin('CAD'), F.col('spend') * 0.70)\n",
    "     .when(F.col('account_currency').isin('EUR'), F.col('spend') * 1.05)\n",
    "     .when(F.col('account_currency').isin('THB'), F.col('spend') * 0.03)\n",
    "     .when(F.col('account_currency').isin('INR'), F.col('spend') * 0.012)\n",
    "     .when(F.col('account_currency').isin('USD'), F.col('spend') * 1.00)\n",
    "     .otherwise(F.col('spend') * 1.00)\n",
    ")\n",
    "print('Converted spend to usd')\n",
    "# print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff25851-6d90-456c-a10e-b4deb7134af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------------+-----+\n",
      "|goal                         |account_currency|count|\n",
      "+-----------------------------+----------------+-----+\n",
      "|IMPRESSIONS                  |THB             |1    |\n",
      "|LINK_CLICKS                  |USD             |17   |\n",
      "|MESSAGING_PURCHASE_CONVERSION|THB             |269  |\n",
      "|MESSAGING_PURCHASE_CONVERSION|USD             |1    |\n",
      "|OFFSITE_CONVERSIONS          |INR             |34681|\n",
      "|OFFSITE_CONVERSIONS          |EUR             |205  |\n",
      "|OFFSITE_CONVERSIONS          |CAD             |5200 |\n",
      "|OFFSITE_CONVERSIONS          |USD             |16223|\n",
      "|POST_ENGAGEMENT              |THB             |2    |\n",
      "|REACH                        |THB             |1    |\n",
      "|REACH                        |USD             |32   |\n",
      "|REPLIES                      |THB             |417  |\n",
      "|REPLIES                      |CAD             |4040 |\n",
      "|RETURN_ON_AD_SPEND           |USD             |82   |\n",
      "|RETURN_ON_AD_SPEND           |INR             |342  |\n",
      "+-----------------------------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy(F.col('goal'), F.col('account_currency')).count()\\\n",
    "    .orderBy(F.col('goal')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce7fca4-d4cf-4520-997c-ed7da27c9a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (56309, 6)\n",
      "some stats:\n",
      "                date     spend_usd   impressions          click         reach  \\\n",
      "count  5.630900e+04  56309.000000  5.630900e+04   56309.000000  5.630900e+04   \n",
      "mean   1.730340e+09     29.966158  7.318835e+03     306.342166  6.316045e+03   \n",
      "std    5.082494e+06     96.009231  2.551094e+04    1856.726374  2.181607e+04   \n",
      "min    1.696262e+09      0.000000  0.000000e+00       0.000000  0.000000e+00   \n",
      "25%    1.726502e+09      0.840840  1.810000e+02       4.000000  1.530000e+02   \n",
      "50%    1.730909e+09      4.980720  1.394000e+03      32.000000  1.186000e+03   \n",
      "75%    1.734451e+09     18.213720  5.682000e+03     144.000000  4.761000e+03   \n",
      "max    1.738858e+09   5152.530000  1.873861e+06  165274.000000  1.506747e+06   \n",
      "\n",
      "          frequency  \n",
      "count  56309.000000  \n",
      "mean       0.987710  \n",
      "std        0.463990  \n",
      "min        0.000000  \n",
      "25%        1.051282  \n",
      "50%        1.116855  \n",
      "75%        1.203747  \n",
      "max        3.500000  \n"
     ]
    }
   ],
   "source": [
    "__def_geom__ = 'NYC,USA'\n",
    "__def_demog__= [{\"age\":'35-55', \"male\": 0.60, \"female\": 0.37, \"unknown\": 0.03}]\n",
    "__def_goal__ = 'OFFSITE_CONVERSIONS'\n",
    "__def_model_name__=\"_\".join([__def_goal__,])\n",
    "''' wihout updated_time', 'spend_usd', 'impressions', 'click', 'reach', 'frequency '''\n",
    "_goal_df= sdf.select('date','spend_usd', 'impressions', 'click', 'reach', 'frequency')\\\n",
    "            .where(F.col('goal').isin(__def_goal__)) \\\n",
    "            .dropna().orderBy('updated_time').toPandas()\n",
    "\n",
    "print(\"data shape:\", _goal_df.shape)\n",
    "print(\"some stats:\\n\", _goal_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a49a4e7b-82d3-4dc8-951c-cccebd3e328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (56309, 6) y:  (56309,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "__def_spend_col_name__='spend_usd'\n",
    "__feature_col_lst__ = ['date', 'impressions', 'click', 'reach', 'frequency']\n",
    "\n",
    "X = np.array(_goal_df[__feature_col_lst__])\n",
    "y = np.array(_goal_df[__def_spend_col_name__])             \n",
    "\n",
    "# add cluster labels to X\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "clustering_ = KMeans(n_clusters = 3).fit(X)\n",
    "# clustering_ = DBSCAN().fit(X)\n",
    "X = np.append(X, clustering_.labels_.reshape(-1,1), 1)\n",
    "\n",
    "print(\"X: \",X.shape,\"y: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de85ca23-500d-42f8-809c-84400c1bbdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (45047, 6) y_train:  (45047,) \n",
      "X_test:  (11262, 6) y_test:  (11262,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "print(\"X_train: \",X_train.shape, \"y_train: \", y_train.shape,\n",
    "      \"\\nX_test: \", X_test.shape, \"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487836a-2ced-445d-98e0-b9228801836e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
